{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "\n",
    "if \"jbook\" in os.getcwd():\n",
    "    os.chdir(os.path.abspath(os.path.join(\"../..\")))\n",
    "FORCE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AppVoCAI Dataset Cleaning\n",
    "---\n",
    "In the previous section, we analyzed the AppVoCAI dataset, evaluating its validity, completeness, uniqueness, relevance, and adherence to data privacy concerns. This section is about duplicate deleting, language filtering, artifact removing, PII masking, character normalizing, data cleaning. \n",
    "\n",
    "Our data cleaning methodology focuses on addressing critical data quality issues that could undermine the integrity of downstream analyses, while preserving the text as close to its original form as possible. By adopting this conservative approach, we tackle key issues without sacrificing the nuance and representativeness of the data, ensuring the models are presented with rich, authentic input.\n",
    "\n",
    "## Data Cleaning Key Evaluation Questions (KEQs)\n",
    "---\n",
    "Although, this data cleaning approach comprises many of the preprocessing techniques commonly found in the literature {cite}`symeonidisComparativeEvaluationPreprocessing2018`, the following data cleaning approach is motivated by three guiding questions.\n",
    "\n",
    "1. What’s essential to remove, and what can be left intact to preserve meaning?\n",
    "2. How do we best preserve text richness and nuance?\n",
    "3. How can the data cleaning process best exploit model strengths towards optimal model performance?\n",
    "\n",
    "These Key Evaluation Questions (KEQs) crystallized our approach which balances data quality with model sophistication.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    "### Import Libraries\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from genailab.setup import auto_wire_container\n",
    "from genailab.infra.service.spark.pool import SparkSessionPool\n",
    "from genailab.analytics.dqa import DQA\n",
    "from genailab.core.dtypes import DFType\n",
    "from genailab.infra.utils.file.fileset import FileFormat\n",
    "from genailab.flow.dataprep.clean.builder import DataCleaningStageBuilder\n",
    "from genailab.flow.dataprep.dqa.builder import DataQualityAssessmentStageBuilder\n",
    "from genailab.asset.dataset.config import DatasetConfig\n",
    "from genailab.core.flow import PhaseDef, StageDef\n",
    "\n",
    "\n",
    "# Wire container\n",
    "container = auto_wire_container()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning Approach\n",
    "---\n",
    "The following describes our data cleaning process and steps, executed in the order listed. We begin with 'safe' techniques that carry minimal risk of compromising downstream cleaning tasks. For instance, UTF-8 encoding can impact the accuracy of language detection algorithms, especially if characters carry language-specific information. Removing special characters may compromise the detection of Personally Identifiable Information (PII) such as URLs and email addresses. As the process progresses, steps carry a greater impact on the data, its expressiveness, and representation.\n",
    "\n",
    "Our minimalist, task-specific, model-informed, *leave-as-is* data cleaning process unfolds as follows:\n",
    "\n",
    "1. **Relevance**: Observations containing **non-English text or app names** will be removed to maintain linguistic uniformity within the dataset, which is crucial for consistent language-based analysis. \n",
    "2. **Uniqueness**: For duplicate review IDs, our policy for retention is based on review date. Retaining the most recent review prioritizes review recency, timeliness and relevance. Observations containing duplicate review text will be retained to maximize fidelity to customer experience and expression. \n",
    "3. **Privacy**: Personally identifiable information (PII) such as URLs, email addresses and phone numbers detected in the previous section are removed from the dataset to ensure adherence to data privacy and minimal information policies.\n",
    "4. **Validity**: Replace or remove artifacts, patterns, and noise that do not convey meaningful content, and may compromise dataset validity. Remediations include:\n",
    "    1. **Control Characters**: Remove HTML entities (e.g., `&amp;`, `&#39;`), and non-printable characters from the Unicode and ASCII character sets used to control text flow or hardware devices (e.g., newline, tab, or carriage return). These characters have no analytical value and can interfere with text processing.\n",
    "    2. **Diacritics and Accents**:  We normalize accented characters (e.g., converting `é` to `e`) to reduce unnecessary text variation, which simplifies analysis without compromising the meaning of the content.\n",
    "    3. **Elongation**: Elongated words (e.g., \"soooo\") convey emphasis in informal text, which is valuable for sentiment analysis. We use a threshold approach to limit characters that appear four or more times consecutively to a maximum of three (e.g., \"soooo\" becomes \"sooo\"), preserving emphasis while maintaining readability.\n",
    "    4. **Repetition**: Excess character, sequence, word and phrase repetition is reduced, but not eliminated to perserve artifacts that may signal emphasis.\n",
    "    5. **Special Characters**: Excessive special characters can indicate SPAM, emotional intensity, or nonsensical content. We apply a threshold: if special characters make up more than 35% of the review text, the review is removed.     \n",
    "    6. **Trim Whitespace**: We trim excess whitespace from review text.\n",
    "5. **Non-Informative Reviews**: Reviews that don't match minimum length criteria are removed.\n",
    "\n",
    "\n",
    "### Data Cleaning Techniques Not Implemented\n",
    "---\n",
    "In natural language processing (NLP), text cleaning measures such as lower-casing, contraction and abbreviation expansion, spelling correction, and the removal of emoticons, emojis, and other artifacts are considered standard practice. Given that transformer models are fine-tuned on large user generated content datasets such as the IMDB Movie and SemEval Laptop Reviews dataset, they are highly adept at handling a wide variety of tokens, including emojis, spelling variations, abbreviations and contractions. So, we take a **leave emojis as-is** approach. By leveraging the inherent strengths of transformer - particularly their ability to tokenize subword units and learn from context — we preserve the natural, authentic nature of user-generated content.\n",
    "\n",
    "## Data Cleaning Pipeline\n",
    "---\n",
    "Following our established pipeline construction process, we specify the source and target dataset configurations and obtain an Apache Spark Session to facilitate data processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Hey Siri, queue the **Data Cleaning** playlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source Dataset Configuration\n",
    "source_config = DatasetConfig(\n",
    "    phase=PhaseDef.DATAPREP,\n",
    "    stage=StageDef.DQA,\n",
    "    name=\"review\",\n",
    "    file_format=FileFormat.PARQUET,\n",
    "    asset_type=\"dataset\",\n",
    "    dftype=DFType.SPARK,\n",
    ")\n",
    "\n",
    "# Target Dataset Configuration\n",
    "target_config = DatasetConfig(\n",
    "    phase=PhaseDef.DATAPREP,\n",
    "    stage=StageDef.SEMICLEAN,\n",
    "    name=\"review\",\n",
    "    file_format=FileFormat.PARQUET,\n",
    "    asset_type=\"dataset\",\n",
    "    dftype=DFType.SPARK,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create builder\n",
    "cleaning_stage = (\n",
    "    DataCleaningStageBuilder()\n",
    "    .clean_non_english()\n",
    "    .clean_privacy_issues()\n",
    "    .clean_duplication()\n",
    "    .clean_invalid_values()\n",
    "    .clean_elongation(threshold=3, max_elongation=2)\n",
    "    .clean_special_chars()\n",
    "    .clean_invalid_characters()\n",
    "    .clean_excess_special_chars(threshold=0.35)\n",
    "    .clean_repeated_words(threshold=3, max_repetitions=1)\n",
    "    .clean_repeated_sequences(length_of_sequence=3, threshold=3, max_repetitions=3)\n",
    "    .clean_repeated_phrases(length_of_phrase=2, threshold=2, max_repetitions=2)\n",
    "    .clean_short_reviews(threshold=5)\n",
    "    .clean_excess_whitespace()\n",
    "    .build(source_config=source_config, target_config=target_config)\n",
    ")\n",
    "# Run the stage\n",
    "dataset = cleaning_stage.run(force=FORCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Verification Pipeline\n",
    "---\n",
    "This data quality verification pipeline will assess the degree to which the cleaned dataset meets quality criteria for validity, relevance, completeness, privacy and uniquness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source Dataset Configuration\n",
    "source_config = DatasetConfig(\n",
    "    phase=PhaseDef.DATAPREP,\n",
    "    stage=StageDef.SEMICLEAN,\n",
    "    name=\"review\",\n",
    "    file_format=FileFormat.PARQUET,\n",
    "    asset_type=\"dataset\",\n",
    "    dftype=DFType.SPARK,\n",
    ")\n",
    "\n",
    "# Target Dataset Configuration\n",
    "target_config = DatasetConfig(\n",
    "    phase=PhaseDef.DATAPREP,\n",
    "    stage=StageDef.DQV,\n",
    "    name=\"review\",\n",
    "    file_format=FileFormat.PARQUET,\n",
    "    asset_type=\"dataset\",\n",
    "    dftype=DFType.SPARK,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create builder\n",
    "builder = DataQualityAssessmentStageBuilder()\n",
    "dqv_stage = (\n",
    "    builder\n",
    "    .detect_non_english()\n",
    "    .detect_privacy_issues()\n",
    "    .detect_duplication()\n",
    "    .detect_invalid_values()\n",
    "    .detect_elongation(threshold=3, max_elongation=2)\n",
    "    .detect_special_chars()\n",
    "    .detect_invalid_characters()\n",
    "    .detect_excess_special_chars(threshold=0.35)\n",
    "    .detect_repeated_words(threshold=3, max_repetitions=1)\n",
    "    .detect_repeated_sequences(threshold=3, max_repetitions=3)\n",
    "    .detect_repeated_phrases(length_of_phrase=2, threshold=2, max_repetitions=2)\n",
    "    .detect_short_reviews(threshold=5)\n",
    "    .detect_excess_whitespace()\n",
    "    .build(source_config=source_config, target_config=target_config)\n",
    ")\n",
    "\n",
    "# Run the stage\n",
    "dataset = dqv_stage.run(force=FORCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Verification\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data Quality Analysis is performed in Pandas, so let's get a Pandas version of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Dataset\n",
    "repo = container.io.repo()\n",
    "target = repo.get(asset_id=dataset.asset_id, dftype=DFType.PANDAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the Data Quality Verification\n",
    "dqa = DQA(dataset=target)\n",
    "dqa.analyze_quality()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality Verification Impressions\n",
    "---\n",
    "The data quality verification reveals strong performance across key dimensions. Completeness reflects some variation in app category coverage, which will be addressed during pre-training and instance selection. Validity confirms adherence to expected formats and rules, ensuring a high signal-to-noise ratio in the text. Relevance is achieved by filtering out non-English reviews and those too short to provide meaningful information for aspect-based sentiment analysis. The dataset is free from duplication, and high privacy compliance is maintained through the removal of personally identifiable information.\n",
    "\n",
    "Finally, we approve the dataset, elevating its status from `semiclean` to `clean`, and persisting it to the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = cleaning_stage.approve(dataset=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark Session\n",
    "container.spark.session_pool().stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
