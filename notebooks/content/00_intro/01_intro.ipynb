{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if \"jbook\" in os.getcwd():\n",
    "    os.chdir(os.path.abspath(os.path.join(\"../..\")))\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GenAI Lab: Exploring Price-Performance of Custom LLMs & SLMs Developed on Consumer Hardware**  \n",
    "\n",
    "Large Language Models (LLMs) are rapidly transforming how businesses drive innovation, analyze customer voice, revealing insights at an unprecedented scale, ultimately leading to faster time-to-value, bolstered profitability, and sustained shareholder value.\n",
    "\n",
    "Their ability to understand and generate human-like text has led to breakthroughs in various applications, from customer service, to content creation and product innovation. \n",
    "\n",
    "## **Generative AI Impact on Product Innovation**\n",
    "Key ways Generative AI impacts product innovation include:\n",
    "\n",
    "**1. Accelerated Ideation**  \n",
    "By analyzing vast datasets of customer feedback, market trends, and competitor benchmarks, Generative AI generates novel product concepts, sparking fresh ideas and uncovering opportunities that might have been missed through manual analysis alone.    \n",
    "\n",
    "**2. Rapid Prototyping**   \n",
    "Generative AI facilitates the creation of digital prototypes based on initial concepts, enabling teams to explore and iterate on designs quickly. This reduces the time and cost associated with traditional prototyping methods, accelerating the journey from idea to execution.   \n",
    "\n",
    "**3. Informed Design Exploration**  \n",
    "AI-generated insights empower teams to explore a wider design space, evaluating diverse possibilities to identify innovative solutions that align with user preferences and market demands.  \n",
    "\n",
    "**4. Data-Driven Insights**  \n",
    "By synthesizing user data and market trends, Generative AI ensures that product development decisions are grounded in evidence, leading to solutions that resonate with customers and outperform competitors.\n",
    "\n",
    "**5. Cost Optimization**  \n",
    "Streamlining the ideation and design process reduces reliance on physical prototypes and extensive manual iterations, driving significant cost savings without compromising creativity or quality.  \n",
    "\n",
    "---\n",
    "\n",
    "However, the widespread adoption of LLMs faces significant challenges.  These include the substantial computational resources required for training and deployment, the high costs associated with accessing and fine-tuning pre-trained models, and the expertise gap hindering many from effectively leveraging this powerful technology.  Furthermore, the complexity of fine-tuning large models often necessitates specialized infrastructure and deep technical knowledge.  This project aims to address these challenges by exploring the potential of custom, smaller language models (SLMs) that can be trained and deployed on consumer-grade hardware.  The central goal is to evaluate the extent to which these custom LLMs/SLMs can approach the performance of fine-tuned foundational models, but at a fraction of the cost, timeframe, and complexity.  By democratizing access to powerful NLP tools, we hope to unlock the potential of LLMs for a wider audience.\n",
    "\n",
    "This project is structured into four distinct phases. Phase 1, DataPrep, focuses on preparing the data for model training. This includes crucial steps like preprocessing, data quality analysis, data cleaning, and text quality analysis, specifically tailored for Aspect-Based Sentiment Analysis (ABSA) and review-level sentiment analysis. Phase 2 delves into Fine-Tuned LLM development and fine-tuning.  This phase explores an unsupervised approach, leveraging pseudo-labeling to iteratively fine-tune a pre-trained LLM.  Phase 3 centers on Custom LLM Development, Training, and Evaluation. Here, we will design, train, and rigorously evaluate our custom SLMs, comparing their performance to the fine-tuned LLM from Phase 2. Finally, Phase 4 will synthesize the Key Findings, highlighting the Strengths and Limitations of our experimental approach.  We will discuss the implications of our results, focusing on the trade-offs between performance, cost, and complexity when using custom SLMs versus fine-tuned LLMs.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## **Applications Across Industries**  \n",
    "\n",
    "Generative AI’s versatility makes it a valuable tool across a range of industries:  \n",
    "\n",
    "- **Consumer Goods**: Developing new product flavors, packaging designs, or concepts that align with consumer preferences.  \n",
    "- **Electronics**: Designing user interfaces or innovating smart device features based on behavioral insights.  \n",
    "- **Automotive**: Creating aerodynamic and aesthetically appealing car designs tailored to consumer preferences.  \n",
    "- **Fashion**: Generating clothing designs aligned with market trends and brand identity.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Challenges of Applying Generative AI to Product Development**\n",
    "While Generative AI (GenAI) offers transformative potential in product development, its application is not without significant challenges. These challenges often stem from the interplay between the sophistication of AI models and the specific requirements of product development processes. Key technical, operational, and organizational challenges include:\n",
    "\n",
    "---\n",
    "\n",
    "### **Technical Challenges**\n",
    "**1. Lack of Domain-Specific Models**  \n",
    "Most pre-trained Generative AI models are designed for general-purpose tasks and often lack the nuanced understanding required for domain-specific applications. For example:\n",
    "- **Generic Outputs**: Models may generate ideas or prototypes that do not align with industry-specific constraints, such as regulatory standards or functional requirements.\n",
    "- **Specialization Gaps**: Tailoring a model for specialized domains, like aerospace design or pharmaceutical formulation, requires additional data and training effort.\n",
    "\n",
    "**2. Technical Complexity**    \n",
    "- **Model Fine-Tuning**: Customizing LLMs to a specific domain requires expertise in techniques such as fine-tuning, reinforcement learning with human feedback (RLHF), and domain adaptation.\n",
    "- **Hyperparameter Optimization**: Effective pretraining or fine-tuning necessitates fine-grained adjustments to hyperparameters, requiring a deep understanding of machine learning principles.\n",
    "- **Data Engineering**: Preparing high-quality datasets for training involves expertise in data cleaning, augmentation, and annotation—all of which are critical for effective domain-specific learning.\n",
    "\n",
    "**3. Data Challenges**\n",
    "- **Data Preparation**: Domain-specific data must often be curated, cleaned, and labeled, which can be both time-consuming and expensive.\n",
    "- **Data Scarcity**: In some industries, there is limited access to high-quality, labeled datasets for training and fine-tuning.\n",
    "- **Data Privacy**: Sharing sensitive or proprietary data with external AI providers poses significant risks to intellectual property and compliance.\n",
    "- **Bias in Data**: Generative models trained on biased datasets may produce outputs that fail to meet inclusivity or accuracy standards.\n",
    "\n",
    "**4. Limited Transferability Across Domains**\n",
    "Even when fine-tuned, a model optimized for one domain often struggles to generalize to another without additional retraining or adaptation. This makes cross-industry applications less efficient and more resource-intensive.\n",
    "\n",
    "---\n",
    "\n",
    "### **Operational Challenges**\n",
    "**5. Cost and Complexity of Customization**\n",
    "Customizing Large Language Models (LLMs) to meet domain-specific needs is resource-intensive:\n",
    "- **High Costs**: Fine-tuning or training LLMs on proprietary data demands substantial computational power, often requiring cloud-based GPU or TPU clusters.\n",
    "- **Inference Costs**: Even without fine-tuning, querying large models at scale can become prohibitively expensive for organizations.\n",
    "- **Access Barriers**: Advanced models like GPT-4 or Llama2 often have licensing restrictions or high usage fees.\n",
    "\n",
    "**6. Interpretability and Explainability**\n",
    "Generative models often function as \"black boxes,\" making it difficult to:\n",
    "- Understand how specific outputs are generated.\n",
    "- Ensure alignment with business objectives or technical constraints.\n",
    "- Validate compliance with regulatory requirements, particularly in highly regulated industries like healthcare or finance.\n",
    "\n",
    "**7. Unpredictable and Inconsistent Results**\n",
    "- Generative AI models may produce outputs that lack relevance or feasibility, requiring human intervention to filter or refine ideas.\n",
    "- Iterative processes become inefficient when the model's creativity veers into unrealistic or impractical territory.\n",
    "\n",
    "**8. Integration with Existing Processes**\n",
    "Integrating AI-generated insights into established product development workflows can be a challenge:\n",
    "- **Workflow Disruption**: Traditional processes may need to be restructured to incorporate AI-driven ideation and prototyping.\n",
    "- **Resistance to Change**: Teams accustomed to conventional methods may struggle to adapt to AI-enabled tools.\n",
    "- **Tooling Gaps**: Many existing design and prototyping platforms are not natively compatible with AI-generated outputs.\n",
    "\n",
    "**9. Risk of Missteps**\n",
    "- **Overfitting**: Without proper expertise, customized LLMs may overfit to niche datasets, reducing their generalizability and effectiveness.\n",
    "- **Inefficient Resource Use**: A lack of skilled practitioners can lead to suboptimal use of expensive computational resources, inflating costs without delivering proportionate value.\n",
    "- **Model Instability**: Poorly executed customizations can lead to unstable or unreliable models, undermining confidence in AI-generated outputs.\n",
    "\n",
    "---\n",
    "\n",
    "### **Organizational Challenges**\n",
    "**10. Specialized Skills Required to Customize and Pretrain LLMs**  \n",
    "One of the significant barriers to leveraging Generative AI for product development lies in the specialized expertise needed to effectively customize and pretrain Large Language Models (LLMs). This challenge is multi-faceted and includes both technical and organizational dimensions:\n",
    "\n",
    "**11. Tooling and Framework Knowledge**\n",
    "- **Deep Learning Frameworks**: Engineers must be proficient in frameworks like PyTorch, TensorFlow, or Hugging Face Transformers, which have steep learning curves.\n",
    "- **Distributed Training**: Pretraining or fine-tuning large models often requires knowledge of distributed computing and parallelism strategies (e.g., data, pipeline, or tensor parallelism).\n",
    "- **Infrastructure Management**: Managing GPU/TPU clusters, ensuring efficient memory usage, and optimizing training pipelines require advanced technical skills.\n",
    "\n",
    "**12. Cross-Disciplinary Expertise**\n",
    "- **Domain Knowledge**: Customizing LLMs for specific industries requires collaboration between AI practitioners and domain experts to encode relevant knowledge into the model.\n",
    "- **Linguistic Proficiency**: For applications involving multilingual or culturally specific data, expertise in linguistics or sociolinguistics is often necessary to ensure model outputs are appropriate and accurate.\n",
    "- **UX and Design**: Effective integration of Generative AI into product development workflows demands knowledge of user experience design and human-computer interaction.\n",
    "\n",
    "**13. Resource-Intensive Learning Curve**\n",
    "- **Continuous Learning**: The rapid pace of innovation in LLMs necessitates ongoing education in new architectures, training paradigms, and optimization techniques (e.g., LoRA, quantization, or adapters).\n",
    "- **Team Composition**: Organizations must assemble multi-disciplinary teams, often competing for scarce talent with specialized AI expertise, leading to high recruitment and retention costs.\n",
    "\n",
    "**14. Organizational Implications**\n",
    "- **Dependency on Specialists**: Relying on a small pool of experts creates bottlenecks in workflows, slowing down innovation and increasing operational risk if key personnel leave.\n",
    "- **High Compensation Costs**: Engineers and researchers skilled in LLM customization command premium salaries, further driving up the cost of deploying generative AI.\n",
    "- **Knowledge Silos**: Specialized skills can lead to knowledge silos, making it difficult for teams to work collaboratively across disciplines.\n",
    "\n",
    "These challenges underscore the complexity of leveraging Generative AI in product development, where technical barriers, operational demands, and organizational hurdles intersect.\n",
    "\n",
    "---\n",
    "\n",
    "## **Addressing the Challenges with a Generative AI-Powered Solution**  \n",
    "This project seeks to address some of the challenges of applying Generative AI in product development through two primary objectives:  \n",
    "\n",
    "1. **Creating a Roadmap for Generative AI Adoption**  \n",
    "   By outlining a clear, practical framework for integrating Generative AI into product development workflows, this project provides actionable guidance on overcoming technical, operational, and organizational barriers. It focuses on how businesses can leverage AI to accelerate innovation, and navigate customization challenges. Phase one of this project: *Discovering unmet opportunities in the Mobile App Market leveraging user review data from the Apple App Store*.\n",
    "\n",
    "2. **Exploring the Viability of Custom Domain-Specific LLMs**  \n",
    "   The project investigates whether smaller, custom Large Language Models (LLMs), trained in an unsupervised context on domain-specific data, can rival or approach the performance of larger, general-purpose models. This evaluation aims to demonstrate how targeted training on specialized datasets can reduce the cost and complexity of AI implementation while maintaining competitive performance, making advanced AI tools accessible to a broader range of organizations.  \n",
    "\n",
    "By addressing these objectives, the project aims to serve as a practical blueprint for leveraging Generative AI effectively while evaluating its potential to democratize access to high-impact AI solutions in product development.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Defining Success**  \n",
    "Success in this project will be measured across two dimensions:  \n",
    "\n",
    "- **Market Opportunity Discovery**:\n",
    "  - The depth and relevance of insights generated from app reviews.\n",
    "  - Identification of actionable, unmet customer needs.\n",
    "  - The ability to inform strategic decisions in real-world scenarios.\n",
    "\n",
    "- **LLM Feasibility**:\n",
    "  - The comparative performance of the custom LLM in extracting actionable insights.\n",
    "  - Efficiency in computational and financial costs.\n",
    "  - Scalability and practicality for resource-constrained teams.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**  \n",
    "This project proposes a faster, more cost-effective, and user-aligned approach to new product innovation. At the same time, by demonstrating the feasibility of custom, low-resource LLMs, we hope to democratize access to these transformative tools, making them accessible to organizations of all sizes.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "appvocai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
