{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if \"jbook\" in os.getcwd():\n",
    "    os.chdir(os.path.abspath(os.path.join(\"../..\")))\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "FORCE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification\n",
    "This stage leverages a **DistilBERT-based Sentiment Classification Model**, specifically the `tabularisai/robust-sentiment-analysis` model, to perform sentiment analysis. The goal is to efficiently analyze and classify sentiment within a dataset for the purposes of **Data Quality Assessment (DQA)** and **Exploratory Data Analysis (EDA)**. \n",
    "\n",
    "## Model Overview\n",
    "- **Model Name**: `tabularisai/robust-sentiment-analysis`\n",
    "- **Base Model**: `distilbert/distilbert-base-uncased`\n",
    "- **Task**: Text Classification (Sentiment Analysis)\n",
    "- **Language**: English\n",
    "- **Number of Classes**: 5 sentiment categories:\n",
    "  - **Very Negative**\n",
    "  - **Negative**\n",
    "  - **Neutral**\n",
    "  - **Positive**\n",
    "  - **Very Positive**\n",
    "\n",
    "## Model Description\n",
    "This model is a fine-tuned version of `distilbert-base-uncased`, optimized for sentiment analysis using synthetic data generated by cutting-edge language models like **Llama3.1** and **Gemma2**. By training exclusively on synthetic data, the model has been exposed to a diverse range of sentiment expressions, which enhances its ability to generalize across different use cases\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from discover.setup import auto_wire_container\n",
    "from discover.flow.stage.model.sentiment import SentimentClassificationStage\n",
    "from discover.core.flow import PhaseEnum, DataEnrichmentStageEnum\n",
    "from discover.infra.config.flow import FlowConfigReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Register `tqdm` with pandas\n",
    "tqdm.pandas()\n",
    "# Wire container\n",
    "container = auto_wire_container()\n",
    "# Pandas\n",
    "pd.options.display.max_colwidth = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Classification Task\n",
    "The `SentimentClassificationTask` class performs sentiment analysis on text data using the `tabularisai/robust-sentiment-analysis` pre-trained transformer model. It is built to handle large-scale text data efficiently and is optimized for execution on GPU when available.\n",
    "\n",
    "**Key Technical Aspects**:\n",
    "\n",
    "1. **Model Loading**: The transformer is loaded using the Hugging Face `transformers` library, leveraging both the `AutoTokenizer` for text tokenization and `AutoModelForSequenceClassification` for sentiment classification.\n",
    "2. **Hardware Optimization**: The class supports GPU acceleration through PyTorch. It checks for the availability of a CUDA-compatible GPU and moves the model and data to the GPU if available. This significantly speeds up inference, making it suitable for large datasets.\n",
    "3. **Text Preprocessing and Tokenization**: Text data is preprocessed and tokenized using the `AutoTokenizer`, which converts text into input tensors that the model can process. The inputs are truncated or padded to a maximum sequence length of 512 tokens, ensuring consistency in input size.\n",
    "4. **Memory Management**: The class uses `torch.cuda.empty_cache()` to clear CUDA memory before loading the model, optimizing memory usage and preventing potential out-of-memory errors on the GPU.\n",
    "5. **Sentiment Prediction**: The `predict_sentiment` method performs inference using `torch.no_grad()` to disable gradient calculation, reducing memory consumption and speeding up computations. It calculates class probabilities using the `softmax` function and maps the predicted class index to a sentiment label.\n",
    "6. **Caching Mechanism**: The class constructs a cache file path using environment-specific settings, making it possible to store and reuse sentiment analysis results efficiently. This can help avoid redundant computations and improve the overall performance of the data pipeline.\n",
    "7. **Integration with DataFrames**: The class operates on pandas DataFrames, applying sentiment analysis to each entry in the specified text column using the `progress_apply` method, which provides a progress bar for monitoring the processing status.\n",
    "\n",
    "The code is included in the following expandable cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# %load -r 19-210 discover/flow/task/model/sentiment.py\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from discover.flow.task.base import Task\n",
    "from discover.infra.service.logging.task import task_logger\n",
    "from discover.infra.utils.file.io import IOService\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------ #\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------ #\n",
    "class SentimentClassificationTask(Task):\n",
    "    \"\"\"\n",
    "    Task for performing sentiment analysis on text data in a specified column of a Pandas DataFrame.\n",
    "\n",
    "    This task uses a pre-trained model to predict sentiment for text in the specified column and\n",
    "    stores the sentiment predictions in a new column. Results are cached to a file to avoid reprocessing.\n",
    "    It supports execution on GPUs or local devices depending on the configuration.\n",
    "\n",
    "    Args:\n",
    "        cache_filepath (str): Path to the cache file for storing or loading sentiment predictions.\n",
    "        column (str): The name of the column in the DataFrame containing text data for sentiment analysis.\n",
    "            Defaults to \"content\".\n",
    "        new_column (str): The name of the column to store sentiment predictions. Defaults to \"sentiment\".\n",
    "        model_name (str): The name of the pre-trained model to use for sentiment analysis. Defaults to\n",
    "            \"tabularisai/robust-sentiment-analysis\".\n",
    "        device_local (bool): Indicates whether to execute the task on local devices. Defaults to False.\n",
    "\n",
    "    Methods:\n",
    "        run(data: pd.DataFrame) -> pd.DataFrame:\n",
    "            Executes the sentiment analysis task, using a cache if available. If not, it predicts sentiment\n",
    "            for the text column and caches the results.\n",
    "        predict_sentiment(text: str) -> str:\n",
    "            Predicts sentiment for a given text string.\n",
    "        _load_model_tokenizer_to_device() -> None:\n",
    "            Loads the model, tokenizer, and device for performing sentiment analysis.\n",
    "        _run(data: pd.DataFrame) -> pd.DataFrame:\n",
    "            Executes the model inference for sentiment prediction and writes the results to the cache.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cache_filepath: str,\n",
    "        column=\"content\",\n",
    "        new_column=\"sentiment\",\n",
    "        model_name: str = \"tabularisai/robust-sentiment-analysis\",\n",
    "        device_local: bool = False,\n",
    "        io_cls: type[IOService] = IOService,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self._column = column\n",
    "        self._new_column = f\"{self.stage.id}_{new_column}\"\n",
    "        self._model_name = model_name\n",
    "        self._cache_filepath = cache_filepath\n",
    "        self._device_local = device_local\n",
    "        self._io = io_cls()\n",
    "\n",
    "        # Model, tokenizer, and device are initialized as None and will be loaded later\n",
    "        self._model = None\n",
    "        self._tokenizer = None\n",
    "        self._device = None\n",
    "\n",
    "    @task_logger\n",
    "    def run(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Executes the sentiment analysis task on the input DataFrame.\n",
    "\n",
    "        This method first attempts to read sentiment predictions from a cache file. If the cache\n",
    "        is not available or not valid, it performs sentiment analysis using the pre-trained model\n",
    "        and writes the results to the cache. Sentiment predictions are stored in the specified\n",
    "        `new_column` of the DataFrame.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): The input DataFrame containing the text data.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The DataFrame with sentiment predictions added to the specified column.\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If the cache is not found or the task is run locally without a GPU.\n",
    "            Exception: For any other unexpected errors.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            cache = self._io.read(filepath=self._cache_filepath, lineterminator=\"\\n\")\n",
    "            cache[\"id\"] = cache[\"id\"].astype(\"string\")\n",
    "            data = data.merge(cache[[\"id\", self._new_column]], how=\"left\", on=\"id\")\n",
    "            return data\n",
    "        except (FileNotFoundError, TypeError):\n",
    "            if self._device_local:\n",
    "                return self._run(data=data)\n",
    "            else:\n",
    "                msg = (\n",
    "                    f\"Cache not found or not available. {self.__class__.__name__} is not \"\n",
    "                    \"supported on local devices. Try running on Kaggle, Colab, or AWS.\"\n",
    "                )\n",
    "                self._logger.error(msg)\n",
    "                raise FileNotFoundError(msg)\n",
    "        except Exception as e:\n",
    "            msg = f\"Unknown exception encountered.\\n{e}\"\n",
    "            self._logger.exception(msg)\n",
    "            raise\n",
    "\n",
    "    def _run(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Executes model inference for sentiment analysis and writes results to the cache.\n",
    "\n",
    "        This method processes the input DataFrame by applying sentiment predictions for each entry\n",
    "        in the specified text column. It uses parallel processing for efficient computation and\n",
    "        writes the results to the cache file.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): The input DataFrame containing the text data.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The DataFrame with sentiment predictions added to the specified column.\n",
    "        \"\"\"\n",
    "        torch.cuda.empty_cache()  # Clear CUDA memory to ensure sufficient space\n",
    "\n",
    "        # Load the device, model, and tokenizer\n",
    "        self._load_model_tokenizer_to_device()\n",
    "\n",
    "        # Apply sentiment prediction to each text entry\n",
    "        data[self._new_column] = data[self._column].progress_apply(\n",
    "            self.predict_sentiment\n",
    "        )\n",
    "\n",
    "        # Write results to the cache file\n",
    "        self._write_file(\n",
    "            filepath=self._cache_filepath, data=data[[\"id\", self._new_column]]\n",
    "        )\n",
    "\n",
    "        return data\n",
    "\n",
    "    def predict_sentiment(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Predicts the sentiment of a given text string.\n",
    "\n",
    "        This method uses the loaded model and tokenizer to predict the sentiment of the input\n",
    "        text. It maps the model's output to a sentiment label.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text string.\n",
    "\n",
    "        Returns:\n",
    "            str: The predicted sentiment label, e.g., \"Positive\", \"Negative\", or \"Neutral\".\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            inputs = self._tokenizer(\n",
    "                text.lower(),\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=512,\n",
    "            )\n",
    "            inputs = {key: value.to(self._device) for key, value in inputs.items()}\n",
    "            outputs = self._model(**inputs)\n",
    "            probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "\n",
    "        sentiment_map = {\n",
    "            0: \"Very Negative\",\n",
    "            1: \"Negative\",\n",
    "            2: \"Neutral\",\n",
    "            3: \"Positive\",\n",
    "            4: \"Very Positive\",\n",
    "        }\n",
    "        return sentiment_map[predicted_class]\n",
    "\n",
    "    def _load_model_tokenizer_to_device(self) -> None:\n",
    "        \"\"\"\n",
    "        Loads the pre-trained model, tokenizer, and device for sentiment analysis.\n",
    "\n",
    "        This method selects the appropriate device (GPU or CPU), loads the tokenizer and model\n",
    "        based on the specified model name, and moves the model to the selected device.\n",
    "        \"\"\"\n",
    "        self._device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(self._model_name)\n",
    "        self._model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self._model_name\n",
    "        )\n",
    "        self._model.to(self._device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Classification Pipeline\n",
    "Similar to the previous Ingestion pipeline, we obtain the configuration using `FlowConfigReader` and set up the `SentimentClassificationStage` with the specified phase and stage definitions. The stage is then built and executed, with the `asset_id` capturing the resulting data asset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12/25/2024 09:35:46 PM] [ERROR] [discover.infra.persist.object.dao.ShelveDAO] [read] : Asset dataset-00_dataprep-05_clean-review was not found.\n",
      "[12/25/2024 09:35:46 PM] [ERROR] [Stage.run] [wrapper] : Exception occurred in Stage.run called with \n",
      "Asset dataset-00_dataprep-05_clean-review was not found.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/john/miniconda3/envs/appvocai/lib/python3.10/shelve.py\", line 111, in __getitem__\n",
      "    value = self.cache[key]\n",
      "KeyError: 'dataset-00_dataprep-05_clean-review'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/john/projects/appvocai-discover/discover/infra/persist/object/dao.py\", line 60, in read\n",
      "    return db[asset_id]\n",
      "  File \"/home/john/miniconda3/envs/appvocai/lib/python3.10/shelve.py\", line 113, in __getitem__\n",
      "    f = BytesIO(self.dict[key.encode(self.keyencoding)])\n",
      "  File \"/home/john/miniconda3/envs/appvocai/lib/python3.10/dbm/dumb.py\", line 147, in __getitem__\n",
      "    pos, siz = self._index[key]     # may raise KeyError\n",
      "KeyError: b'dataset-00_dataprep-05_clean-review'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/john/projects/appvocai-discover/discover/infra/service/logging/stage.py\", line 55, in wrapper\n",
      "    result = func(self, *args, **kwargs)\n",
      "  File \"/home/john/projects/appvocai-discover/discover/flow/stage/base.py\", line 170, in run\n",
      "    return self._run()\n",
      "  File \"/home/john/projects/appvocai-discover/discover/flow/stage/base.py\", line 179, in _run\n",
      "    data = self._get_data(\n",
      "  File \"/home/john/projects/appvocai-discover/discover/flow/stage/base.py\", line 258, in _get_data\n",
      "    dataset = self._workspace_service.dataset_repo.get(asset_id=asset_id)\n",
      "  File \"/home/john/projects/appvocai-discover/discover/infra/persist/repo/base.py\", line 38, in get\n",
      "    return self._dao.read(asset_id=asset_id)\n",
      "  File \"/home/john/projects/appvocai-discover/discover/infra/persist/object/dao.py\", line 64, in read\n",
      "    raise ObjectNotFoundError(msg)\n",
      "discover.infra.exception.object.ObjectNotFoundError: Asset dataset-00_dataprep-05_clean-review was not found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "# ============================================================================== #\n",
      "#                         Sentiment Classification Stage                         #\n",
      "# ============================================================================== #\n",
      "\n"
     ]
    },
    {
     "ename": "ObjectNotFoundError",
     "evalue": "Asset dataset-00_dataprep-05_clean-review was not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/appvocai/lib/python3.10/shelve.py:111\u001b[0m, in \u001b[0;36mShelf.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dataset-00_dataprep-05_clean-review'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/projects/appvocai-discover/discover/infra/persist/object/dao.py:60\u001b[0m, in \u001b[0;36mShelveDAO.read\u001b[0;34m(self, asset_id)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m shelve\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_db_path) \u001b[38;5;28;01mas\u001b[39;00m db:\n\u001b[0;32m---> 60\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdb\u001b[49m\u001b[43m[\u001b[49m\u001b[43masset_id\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/appvocai/lib/python3.10/shelve.py:113\u001b[0m, in \u001b[0;36mShelf.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     f \u001b[38;5;241m=\u001b[39m BytesIO(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeyencoding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    114\u001b[0m     value \u001b[38;5;241m=\u001b[39m Unpickler(f)\u001b[38;5;241m.\u001b[39mload()\n",
      "File \u001b[0;32m~/miniconda3/envs/appvocai/lib/python3.10/dbm/dumb.py:147\u001b[0m, in \u001b[0;36m_Database.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verify_open()\n\u001b[0;32m--> 147\u001b[0m pos, siz \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m     \u001b[38;5;66;03m# may raise KeyError\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _io\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_datfile, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mKeyError\u001b[0m: b'dataset-00_dataprep-05_clean-review'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mObjectNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Build and run Data Sentiment Analysis Stage\u001b[39;00m\n\u001b[1;32m      8\u001b[0m stage \u001b[38;5;241m=\u001b[39m SentimentClassificationStage\u001b[38;5;241m.\u001b[39mbuild(stage_config\u001b[38;5;241m=\u001b[39mstage_config, force\u001b[38;5;241m=\u001b[39mFORCE)\n\u001b[0;32m----> 9\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/appvocai-discover/discover/infra/service/logging/stage.py:55\u001b[0m, in \u001b[0;36mstage_logger.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m start_fmt \u001b[38;5;241m=\u001b[39m start\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Execute the original function being decorated, passing all args and kwargs.\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Log runtime.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m end \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n",
      "File \u001b[0;32m~/projects/appvocai-discover/discover/flow/stage/base.py:170\u001b[0m, in \u001b[0;36mStage.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workspace_service\u001b[38;5;241m.\u001b[39mdataset_repo\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    167\u001b[0m         asset_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_destination_asset_id\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/appvocai-discover/discover/flow/stage/base.py:179\u001b[0m, in \u001b[0;36mStage._run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Processes the source dataset and applies tasks to generate the destination dataset.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03m    Dataset: The newly created destination dataset.\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# Obtain the source data in the specified dataframe structure\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43masset_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_source_asset_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataframe_structure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_source_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataframe_structure\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# Apply tasks to the data sequentially.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks:\n",
      "File \u001b[0;32m~/projects/appvocai-discover/discover/flow/stage/base.py:258\u001b[0m, in \u001b[0;36mStage._get_data\u001b[0;34m(self, asset_id, dataframe_structure)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_data\u001b[39m(\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m, asset_id: \u001b[38;5;28mstr\u001b[39m, dataframe_structure: DataFrameStructure\n\u001b[1;32m    256\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[pd\u001b[38;5;241m.\u001b[39mDataFrame, DataFrame]:\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;66;03m# Obtain the source dataset and extract the DataFrame in the specified structure.\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_workspace_service\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_repo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43masset_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masset_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mas_df(dataframe_structure\u001b[38;5;241m=\u001b[39mdataframe_structure)\n",
      "File \u001b[0;32m~/projects/appvocai-discover/discover/infra/persist/repo/base.py:38\u001b[0m, in \u001b[0;36mAssetRepo.get\u001b[0;34m(self, asset_id)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, asset_id: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Asset:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dao\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43masset_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masset_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/appvocai-discover/discover/infra/persist/object/dao.py:64\u001b[0m, in \u001b[0;36mShelveDAO.read\u001b[0;34m(self, asset_id)\u001b[0m\n\u001b[1;32m     62\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00masset_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39merror(msg)\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ObjectNotFoundError(msg)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     66\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe object database for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_asset_type\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_db_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mObjectNotFoundError\u001b[0m: Asset dataset-00_dataprep-05_clean-review was not found."
     ]
    }
   ],
   "source": [
    "# Obtain the configuration\n",
    "reader = FlowConfigReader()\n",
    "stage_config = reader.get_stage_config(\n",
    "    phase=PhaseEnum.ENRICHMENT, stage=DataEnrichmentStageEnum.SENTIMENT\n",
    ")\n",
    "\n",
    "# Build and run Data Sentiment Analysis Stage\n",
    "stage = SentimentClassificationStage.build(stage_config=stage_config, force=FORCE)\n",
    "dataset = stage.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Results\n",
    "This sample illustrates sentiment vis-a-vis ratings, revealing the complexity and nuance in user opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_pandas()[[\"id\", \"content\", \"rating\", \"en_sentiment\"]].sample(\n",
    "    n=5, random_state=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Sentiment vs. Ratings\n",
    "1. **Entry 1: Mooncycle**\n",
    "   - **Rating**: 4\n",
    "   - **Sentiment Analysis**: Very Positive\n",
    "   - **Comment**: The user provided a high rating (4 stars), and the sentiment analysis correctly identified a very positive sentiment. This indicates a good match between the expressed sentiment and the user's rating.\n",
    "\n",
    "2. **Entry 2: Privacy Concern**\n",
    "   - **Rating**: 3\n",
    "   - **Sentiment Analysis**: Neutral\n",
    "   - **Comment**: The review mentions significant concerns about privacy features but still gives a moderate rating of 3 stars. The sentiment analysis classified this as Neutral, which seems reasonable given the mix of positive and negative feedback. However, one might argue that a \"Slightly Negative\" label could better capture the overall tone.\n",
    "\n",
    "3. **Entry 3: Survey Payouts**\n",
    "   - **Rating**: 3\n",
    "   - **Sentiment Analysis**: Negative\n",
    "   - **Comment**: The user was disappointed with survey payouts, rating the experience as 3 stars. The sentiment analysis classified this as Negative, which reflects the user's dissatisfaction. The rating, however, seems higher than expected for a purely negative sentiment, suggesting potential leniency or mixed feelings not fully captured by the text.\n",
    "\n",
    "4. **Entry 4: Instagram Censorship**\n",
    "   - **Rating**: 1\n",
    "   - **Sentiment Analysis**: Very Negative\n",
    "   - **Comment**: This review strongly criticizes Instagram's content policies, and the user gave the lowest possible rating (1 star). The sentiment analysis accurately labeled this as Very Negative, showing a clear alignment between sentiment and rating.\n",
    "\n",
    "5. **Entry 5: Informative App**\n",
    "   - **Rating**: 5\n",
    "   - **Sentiment Analysis**: Very Positive\n",
    "   - **Comment**: The review is overwhelmingly positive, emphasizing the app's usefulness and unique features, and the user gave a 5-star rating. The sentiment analysis correctly labeled it as Very Positive, demonstrating alignment between the rating and sentiment.\n",
    "\n",
    "### Observations\n",
    "- **Alignment**: In most cases, the sentiment analysis aligns well with the user ratings. Positive sentiments correlate with higher ratings, while negative sentiments correspond to lower ratings.\n",
    "- **Mixed Reviews**: The Neutral sentiment for the privacy concern review highlights the complexity of mixed feedback, where both positives and negatives are present. This might require more nuanced classification.\n",
    "- **Alignment Between Sentiment and Rating**: In most cases, there is alignment between the sentiment analysis and user ratings. For instance, Very Positive sentiments are generally accompanied by high ratings (4 or 5), and Very Negative sentiments align with the lowest rating of 1.\n",
    "- **Neutral Sentiment vs. Moderate Rating**: For reviews with Neutral or Negative sentiment (Ratings: 3), the ratings reflect appreciation for the app's core value but reveal dissatisfaction with specific features or limitations.\n",
    "- **Sentiment Outliers**: No significant mismatches are observed here, suggesting that the sentiment analysis accurately reflects the reviewerâ€™s stance in this sample. However, cases like Review 2 highlight how neutral sentiments can still accompany moderate ratings due to unfulfilled expectations.\n",
    "\n",
    "This analysis indicates that sentiment analysis can generally align well with user ratings, offering insights into specific areas of dissatisfaction or satisfaction that might otherwise be missed in numerical ratings alone.\n",
    "\n",
    "In the next section, we evaluate data quality and requirements for data cleaning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "appvocai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
