{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if \"jbook\" in os.getcwd():\n",
    "    os.chdir(os.path.abspath(os.path.join(\"../..\")))\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "FORCE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity\n",
    "Perplexity is a measurement used in natural language processing (NLP) to evaluate how well a language model predicts a sequence of words. It quantifies the degree of uncertainty a model has when generating or understanding text. IOW, perplexity tells us how \"perplexed\" or confused a language model is when trying to predict the next word in a sequence.\n",
    "\n",
    "Mathematically, perplexity is the exponential of the average negative log-likelihood of a sequence of words. A lower perplexity indicates that the model has a better understanding of the text and can predict words with greater certainty, while a higher perplexity indicates that the model struggles to predict the text accurately.\n",
    "\n",
    "### Why Use Perplexity as a Proxy for Gibberish Detection?\n",
    "In the context of gibberish detection, perplexity serves as a useful proxy to determine how coherent or meaningful a piece of text is:\n",
    "- **Coherent Text**: Well-formed, grammatically correct text that follows the rules of a language will typically have a lower perplexity because the language model can predict the sequence more easily.\n",
    "- **Gibberish**: Random or nonsensical text, on the other hand, will have a higher perplexity because it is harder for the language model to predict the next word or make sense of the text. The lack of recognizable linguistic patterns or coherence makes the model \"perplexed.\"\n",
    "\n",
    "By using perplexity as a metric, we can identify text that is likely gibberish or low quality, which is particularly valuable in tasks such as data quality assessment, content moderation, or filtering out noise from large text datasets.\n",
    "\n",
    "### How is Perplexity Calculated?\n",
    "Perplexity is calculated using a language model that has been trained on a large corpus of text. Here’s a step-by-step explanation of how it works:\n",
    "\n",
    "1. **Tokenization**: The text is first tokenized into words or subwords that the language model can process.\n",
    "2. **Model Prediction**: The language model assigns a probability to each word in the sequence based on the words that precede it. The likelihood of the entire sequence is then computed as the product of the probabilities of each word.\n",
    "3. **Log-Likelihood**: To make the calculations more manageable, the negative log-likelihood of the sequence is computed.\n",
    "4. **Average Log-Likelihood**: The average negative log-likelihood per word is calculated over the entire sequence.\n",
    "5. **Perplexity**: Finally, perplexity is calculated as the exponential of the average negative log-likelihood:\n",
    "   $$\n",
    "   \\text{Perplexity} = \\exp\\left(-\\frac{1}{N} \\sum_{i=1}^{N} \\log P(w_i)\\right)\n",
    "   $$\n",
    "   where $N$ is the number of words in the text, and $P(w_i)$ is the probability assigned to the $i^{th}$ word by the model.\n",
    "\n",
    "### Interpreting Perplexity\n",
    "- **Low Perplexity**: Indicates that the text is easier for the model to predict, suggesting it is coherent and follows typical language patterns.\n",
    "- **High Perplexity**: Suggests that the text is difficult to predict, often indicating that the text is gibberish, random, or otherwise unconventional.\n",
    "\n",
    "### Why Perplexity Matters\n",
    "Perplexity is a widely used metric in NLP for evaluating language models, and it provides a quantitative way to assess the quality of text. In our analysis, we use perplexity as an indicator to flag potential gibberish or poorly constructed text, which is crucial for filtering and cleaning data in natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from enum import Enum\n",
    "\n",
    "from discover.container import DiscoverContainer\n",
    "from discover.infra.service.datamanager.perplexity import PerplexityAnalysisDataManager\n",
    "\n",
    "# Register `tqdm` with pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "pd.options.display.max_colwidth = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "container = DiscoverContainer()\n",
    "container.init_resources()\n",
    "container.wire(\n",
    "    modules=[\n",
    "        \"discover.infra.service.datamanager.base\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manager\n",
    "The `PerplexityAnalysisDataManager` owns persistence of data and datasets used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamanager = PerplexityAnalysisDataManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Path Options\n",
    "This notebook supports three execution paths:\n",
    "\n",
    "1. **Load Endpoint**: If the notebook has already been executed and results are stored in the repository, they will be loaded. This path is used unless the `FORCE` parameter is set to `True`.\n",
    "2. **Load perplexities**: If perplexity analysis results have been precomputed on cloud-based GPUs and saved in a CSV file, the file will be loaded and merged with the dataset, unless `FORCE` is `True`.\n",
    "3. **Execute Inference**: If `FORCE` is set to `True` or if neither the endpoint nor the perplexity file is available, the notebook will perform inference using the perplexity analysis model.\n",
    "\n",
    "The following code supports the determination of the execution path based on these conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExecutionPath(Enum):\n",
    "    LOAD_ENDPOINT = \"load_endpoint\"\n",
    "    LOAD_PERPLEXITY = \"load_perplexity\"\n",
    "    EXECUTE_INFERENCE = \"execute_inference\"\n",
    "\n",
    "\n",
    "def determine_execution_path(\n",
    "    force: bool, datamanager: PerplexityAnalysisDataManager\n",
    ") -> ExecutionPath:\n",
    "    \"\"\"Determines the execution path based on the existence of data and the force parameter.\n",
    "\n",
    "    Args:\n",
    "        force (bool): Whether to force execution, overriding existing data checks.\n",
    "        data_manager (PerplexityAnalysisDataManager): The data manager to check for existing datasets and perplexities.\n",
    "\n",
    "    Returns:\n",
    "        ExecutionPath: The determined execution path.\n",
    "    \"\"\"\n",
    "    if force:\n",
    "        return ExecutionPath.EXECUTE_INFERENCE\n",
    "\n",
    "    elif datamanager.dataset_exists(stage=\"perplexity\"):\n",
    "        return ExecutionPath.LOAD_ENDPOINT\n",
    "\n",
    "    elif datamanager.perplexity_exist():\n",
    "        return ExecutionPath.LOAD_PERPLEXITY\n",
    "\n",
    "    else:\n",
    "        return ExecutionPath.EXECUTE_INFERENCE\n",
    "\n",
    "\n",
    "execution_path = determine_execution_path(force=FORCE, datamanager=datamanager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Endpoint\n",
    "Loads the endpoint if appropriate given the execution path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if execution_path == ExecutionPath.LOAD_ENDPOINT:\n",
    "    df = datamanager.get_dataset(stage=\"perplexity\", name=\"review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-Computed Perplexities\n",
    "Obtain the dataset from the prior stage, 'ingest', and merge in the perplexities from file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if execution_path == ExecutionPath.LOAD_PERPLEXITY:\n",
    "    df = datamanager.get_dataset(stage=\"sentiment\", name=\"review\")\n",
    "    perplexity = datamanager.get_perplexity()\n",
    "    df = datamanager.merge_perplexity(df=df, perplexity=perplexity)\n",
    "    datamanager.add_dataset(df=df, stage=\"perplexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Inference\n",
    "The following cells perform inference using the perplexity analysis model according to the execution path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Model and Transformer Libraries\n",
    "PyTorch model and tokenizer are imported, as well as tqdm for progress monitoring.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if execution_path == ExecutionPath.EXECUTE_INFERENCE:\n",
    "    from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "    import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU Availability and Prepare for Inference \n",
    "Verify GPU availability, ensuring GPU resources are being detected and utilized. To mitigate memory issues, release all unused cached memory held by the caching allocator, making it available for other GPU applications and visible in `nvidia-smi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if execution_path == ExecutionPath.EXECUTE_INFERENCE:\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"GPU count:\", torch.cuda.device_count())\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    !nvidia-smi\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Loads the data from the ingest stage from the repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if execution_path == ExecutionPath.EXECUTE_INFERENCE:\n",
    "    df = datamanager.get_dataset(stage=\"sentiment\", name=\"review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer\n",
    "Import and load the perplexity analyzer and the tokenizer designed for sequence classification, then move the model to the device detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "if execution_path == ExecutionPath.EXECUTE_INFERENCE:\n",
    "    model_id = \"distilbert/distilgpt2\"\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n",
    "    max_length = model.config.n_positions\n",
    "    stride = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Classifier\n",
    "Tokenize the string of text, truncating it to 512 characters and pad the text if it is shorter than 512 characters. Move the tokenized input to the device detected. Probabilities are computed for each class, and the function returns the highest probability class label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict perplexity\n",
    "def predict_perplexity(text):\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(\n",
    "            text.lower(),\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512,\n",
    "        )\n",
    "        inputs = {\n",
    "            key: value.to(device) for key, value in inputs.items()\n",
    "        }  # Move inputs to the GPU\n",
    "        seq_len = inputs[\"input_ids\"].size(1)\n",
    "        nlls = []\n",
    "        prev_end_loc = 0\n",
    "        for begin_loc in range(0, seq_len, stride):\n",
    "            end_loc = min(begin_loc + max_length, seq_len)\n",
    "            trg_len = (\n",
    "                end_loc - prev_end_loc\n",
    "            )  # may be different from stride on last loop\n",
    "            input_ids = inputs[\"input_ids\"][:, begin_loc:end_loc].to(device)\n",
    "            target_ids = input_ids.clone()\n",
    "            target_ids[:, :-trg_len] = -100\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "                # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "                # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "                # to the left by 1.\n",
    "                neg_log_likelihood = outputs.loss\n",
    "\n",
    "            nlls.append(neg_log_likelihood)\n",
    "\n",
    "            prev_end_loc = end_loc\n",
    "            if end_loc == seq_len:\n",
    "                break\n",
    "    return torch.exp(torch.stack(nlls).mean()).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference\n",
    "Run inference using the classification function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if execution_path == ExecutionPath.EXECUTE_INFERENCE:\n",
    "    df[\"dqp_perplexity\"] = df[\"content\"].progress_apply(predict_perplexity)\n",
    "    datamanager.add_dataset(df=df, stage=\"perplexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"id\", \"app_name\", \"content\", \"rating\", \"dqp_perplexity\"]].sample(\n",
    "    n=5, random_state=22\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this sample, several observations are notable:\n",
    "\n",
    "1. **Ad Block One: Tube Ad Blocker (\"Awesome\")**: The 5-star rating and \"Very Positive\" perplexity remain well-aligned, as the single-word feedback conveys a clear and enthusiastic endorsement. No further action needed here.\n",
    "\n",
    "2. **Cleanup: Phone Storage Cleaner (\"Save time and space\")**: The perplexity analysis again correctly identifies the positive tone of the review, which matches the 5-star rating. The short, impactful statement reflects high user satisfaction with the app's functionality.\n",
    "\n",
    "3. **sweetgreen**: The expanded review content continues to justify the \"Very Positive\" perplexity and the 5-star rating. The user expresses enthusiasm about the app's interface, ease of use, and the quality of the food. Despite suggesting improvements (like adding more bases and supporting Apple Pay), the overall perplexity remains overwhelmingly positive. This is a good example of how constructive feedback can coexist with high satisfaction, and the perplexity analysis accurately captures the overall positive tone.\n",
    "\n",
    "4. **OwO Novel - Read Romance Story**: The mismatch between the negative content and the 5-star rating becomes even more evident with the added details. The user explicitly states that the app \"is not worth 5 stars\" and criticizes the rising cost for chapters. This discrepancy is likely a case where the perplexity model is correct in detecting negativity, but the user gave a high rating that contradicts their review. This case suggests that users may sometimes give ratings that do not reflect their written feedback, highlighting the complexity of relying solely on ratings for perplexity analysis.\n",
    "\n",
    "5. **Bible**: The review content provides constructive feedback alongside a description of regular app use. The suggestions for additional features, like a chronological version and specific plans, are not emotionally charged, which supports the \"Neutral\" perplexity label. However, the 5-star rating indicates a high level of satisfaction despite the neutral tone of the review. This suggests that the user is content overall but expressed feedback in a more factual manner. The model’s labeling is understandable, but incorporating more contextual understanding might help align perplexity labels more closely with ratings in cases like this.\n",
    "\n",
    "### Key Takeaways and Recommendations:\n",
    "- **sweetgreen**: The perplexity analysis does well to capture overall positivity despite the presence of suggestions for improvement, demonstrating robustness in handling mixed feedback.\n",
    "- **OwO Novel - Read Romance Story**: This highlights a potential gap in understanding user intent behind ratings. Further investigation into user behavior (such as high ratings paired with negative comments) may provide insights into refining perplexity analysis models.\n",
    "- **Bible**: This review underscores the challenge of interpreting reviews that are positive overall but expressed in a neutral tone. perplexity analysis might benefit from additional heuristics or metadata to better align with user ratings.\n",
    "\n",
    "Overall, these examples illustrate the complexities of perplexity analysis when ratings and content don’t always align perfectly, but your model appears to be performing well in capturing the general perplexity conveyed by the text. Let me know if you’d like to explore further improvements or adjustments!\n",
    "\n",
    "In the next section, we detect anomalies in the text that *might* introduce *harmful* noise into the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "appvocai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
